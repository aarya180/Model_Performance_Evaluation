{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Evaluation Metrics and Custom Feature Selection\n",
    "Some classical evaluation metrics occasionally fall short under certain circumstances. In the first part of this homework, you will implement a few special evaluation metrics used in predictive analytics that is designated for one-sided and imbalanced prediction tasks.\n",
    "\n",
    "In the second part, you will be implementing a custom feature selection procedure. This will be a univariate feature selection method. You will be given a toy dataset called 'Car Evaluation Data Set' (see: http://archive.ics.uci.edu/ml/datasets/Car+Evaluation for details). You are not required to, but advised to test your code with the toy dataset, or any other dataset that contains categorical variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Forecast Metrics\n",
    "There are many types of forecasts, each calling for slightly different methods of verification. The primary target for the measures below are dichotomous forecasts (in other words, binary predictions). \n",
    "\n",
    "The forecast terminology is slightly different. Following are the differences:\n",
    "- Instead of True Positives (TP) --> _Hits_ (a) is used, \n",
    "- Instead of False Positives (FP) --> _False Alarms_ (b) is used,\n",
    "- Instead of False Negatives (FN) --> _Misses_ (c) is used, \n",
    "- Instead of True Negatives (TN) --> _Correct Rejects_ (d) is used.\n",
    "\n",
    "For every single model run, you are given:\n",
    "1. a set of observations (Event [1] or No Event [0]) \n",
    "2. a set of prediction scores (a float between 0 and 1) and an event threshold, where the predicted outcome will be \n",
    "\n",
    "<center>$\\hat{y}= \n",
    "\\begin{cases}\n",
    "    1 \\text{ (Event occurred)},& \\text{if } score \\geq threshold\\\\\n",
    "    0 \\text{ (No event),}      & \\text{if } score < threshold\n",
    "\\end{cases} $ <center>\n",
    "\n",
    "Note that observations are ground truth and prediction scores and threshold will be used for determining the predicted model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Here are some test data you can use for this homework\n",
    "Y_test = [0, 0, 1, 1, 0, 1, 0, 0, 0, 1] # observations\n",
    "P_scores = [0.1, 0.32, 0.48, 0.9, 0.6, 0.55, 0.42, 0.37, 0.61, 0.66] # prediction scores\n",
    "threshold = 0.5 # prediction threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 1 (10 points)\n",
    "Given prediction scores, threshold, and observations create a function to determine the elements of a confusion matrix. For ease of use, you will output a `dict` (dictionary) object instead of a 2-dimensional numpy array. Note that _positive_ corresponds to the event occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 3, 'TN': 4, 'FP': 2, 'FN': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_conf_matrix(observation, p_scores, threshold):\n",
    "    '''Finds the entries (TP, FP, FN, TN) in a binary confusion matrix for forecast problems\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observation: list\n",
    "        list of observations (1 is there is event, 0 is there is no event)\n",
    "    p_scores: list\n",
    "        list of prediction scores (scores vary between 0 and 1)\n",
    "    threshold: float\n",
    "        threshold that will be used for binary outcome from prediction scores\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        a dictionary that shows the counts for TP, TN, FP, and FNs.\n",
    "    \n",
    "    '''\n",
    "    pass\n",
    "    \n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    p_pred = [1 if x >=threshold else 0 for x in p_scores]\n",
    "    \n",
    "    for i in range(0,len(p_pred)):\n",
    "        if observation[i]==0 and p_pred[i]==0:\n",
    "            TN=TN+1\n",
    "        if observation[i]==0 and p_pred[i]==1:\n",
    "            FP=FP+1\n",
    "        if observation[i]==1 and p_pred[i]==0:\n",
    "            FN=FN+1\n",
    "        if observation[i]==1 and p_pred[i]==1:\n",
    "            TP=TP+1\n",
    "    dict_output={\"TP\":TP,\"TN\":TN,\"FP\":FP,\"FN\":FN}\n",
    "    return(dict_output)\n",
    "    \n",
    "binary_conf_matrix(Y_test, P_scores, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 2 (10 points)\n",
    "\n",
    "Create functions for calculating accuracy, precision, recall, and F1-score. You can use the definitions from slides (Chapter 11). (You are supposed to calculate the precision and recall (and thus F1-score) for 'Event' [1] class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of event class: 0.7\n",
      "Precison Score of event class: 0.6\n",
      "Recall Score of event class: 0.75\n",
      "F-1 Score of event Class: 0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "def accuracy(observation, p_scores, threshold):\n",
    "    pass\n",
    "    \n",
    "    result_dict=binary_conf_matrix(observation, p_scores, threshold) ## Getting Confusion matrix dictionary\n",
    "    \n",
    "    accruracy_score=(result_dict[\"TP\"]+result_dict[\"TN\"])/(result_dict[\"TP\"]+result_dict[\"TN\"]+result_dict[\"FP\"]+result_dict[\"FN\"])\n",
    "    \n",
    "    return accruracy_score \n",
    "    \n",
    "def precision(observation, p_scores, threshold):\n",
    "    pass\n",
    "    result_dict=binary_conf_matrix(observation, p_scores, threshold)   ## Getting Confusion matrix dictionary\n",
    "    precision_score=result_dict[\"TP\"]/(result_dict[\"TP\"]+result_dict[\"FP\"])  ## Precision for Event Class\n",
    "    return precision_score\n",
    "\n",
    "def recall(observation, p_scores, threshold):\n",
    "    pass\n",
    "    result_dict=binary_conf_matrix(observation, p_scores, threshold)  ## Getting Confusion matrix dictionary\n",
    "    recall_score=result_dict[\"TP\"]/(result_dict[\"TP\"]+result_dict[\"FN\"])  ## Recall for Event Class\n",
    "    return recall_score\n",
    "\n",
    "def f1score(observation, p_scores, threshold):\n",
    "    pass\n",
    "    re_score=recall(observation, p_scores, threshold)     ## Getting Recall for event class\n",
    "    pr_score=precision(observation, p_scores, threshold)  ## Precision for Event class\n",
    "    F1_score=2*re_score*pr_score/(re_score+pr_score)\n",
    "    return F1_score\n",
    " \n",
    "## Printing all the scores    \n",
    "print(\"Accuracy score of event class:\",accuracy( Y_test, P_scores, threshold ))\n",
    "print(\"Precison Score of event class:\",precision( Y_test, P_scores, threshold ))\n",
    "print(\"Recall Score of event class:\",recall( Y_test, P_scores, threshold ))\n",
    "print(\"F-1 Score of event Class:\",f1score( Y_test, P_scores, threshold ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 3 (5 points)\n",
    "Calculate the bias score (BIAS). Bias score measures the ratio of the frequency of predicted event occurrences to the frequency of observed events. It can be calculated using the following formula:\n",
    "\n",
    "$ BIAS = \\frac{\\text{hits} + \\text{false alarms} }{ \\text{hits} + \\text{misses} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIAS Score is : 1.25\n"
     ]
    }
   ],
   "source": [
    "def bias_score(observation, p_scores, threshold):\n",
    "    pass\n",
    "    result_dict=binary_conf_matrix(observation, p_scores, threshold)  ## Getting confusion matrix dictionary\n",
    "    BIAS_Score=(result_dict[\"TP\"]+result_dict[\"FP\"])/(result_dict[\"TP\"]+result_dict[\"FN\"])\n",
    "    return(BIAS_Score)\n",
    "    \n",
    "print(\"BIAS Score is :\",bias_score(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 4 (5 points)\n",
    "\n",
    "Calculate the false alarm ratio (FAR). FAR is an indicator of what fraction of the predicted events actually did not occur (i.e., they were false alarms)? You can calculate the FAR as follows: \n",
    "\n",
    "$ FAR = \\frac{ \\text{false alarms} }{ \\text{hits} + \\text{false alarms} }  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The False alarm ratio will be : 0.4\n"
     ]
    }
   ],
   "source": [
    "def far(observation, p_scores, threshold):\n",
    "    pass\n",
    "    result_dict=binary_conf_matrix(observation, p_scores, threshold) ## Getting confusion matrix dictionary\n",
    "    FAR= (result_dict[\"FP\"])/(result_dict[\"FP\"]+result_dict[\"TP\"])\n",
    "    return(FAR)\n",
    "    \n",
    "\n",
    "\n",
    "print(\"The False alarm ratio will be :\",far(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 5 (5 points)\n",
    "Calculate the threat score. Threat score (which is also referred to as critical success index -- CSI) indicates how well did the predicted event outcomes correspond to the observed events. It measures the fraction of actual __and/or__ predicted events that were correctly predicted. It can be thought of as the accuracy when the correct negatives (TN) have been removed from consideration. It can be calculated as follows:\n",
    "\n",
    "$CSI = \\frac{ \\text{hits} }{ \\text{hits} + \\text{misses} + \\text{false alarms} } $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSI Scores will be :  0.5\n"
     ]
    }
   ],
   "source": [
    "def csi(observation, p_scores, threshold):\n",
    "    pass\n",
    "    result_dict=binary_conf_matrix(observation, p_scores, threshold)  ## Getting confusion matrix dictionary\n",
    "    CSI=(result_dict[\"TP\"])/(result_dict[\"TP\"]+result_dict[\"FN\"]+result_dict[\"FP\"])\n",
    "    return CSI\n",
    "\n",
    "print(\"CSI Scores will be : \",csi(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 6 (7.5 points)\n",
    "Skill scores are often used to understand the improvement of model performance over a given baseline (often a hypothetical, predetermined random forecast result). The first skill score you will implement is _true skill statistic (TSS)_ (which is also known as Hanssen-Kuipers Skill Score. It can be used to understand how well the prediction model separate the events (detected) from the no events. TSS can be calculated as follows:\n",
    "\n",
    "$ TSS = \\frac{\\text{hits} }{\\text{hits} + \\text{misses}} - \\frac{\\text{false alarms} }{\\text{false alarms} + \\text{correct negatives}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS Scores will be :  0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "def tss(observation, p_scores, threshold):\n",
    "    pass\n",
    "    result_dict=binary_conf_matrix(observation, p_scores, threshold)  ## Getting confusion matrix dictionary\n",
    "    TSS= (result_dict[\"TP\"]/(result_dict[\"TP\"]+result_dict[\"FN\"]))-(result_dict[\"FP\"]/(result_dict[\"FP\"]+result_dict[\"TN\"]))\n",
    "    return TSS\n",
    "print(\"TSS Scores will be : \",tss(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 7 (7.5 points)\n",
    "The next skill score to calculate is Gilbert Skill Score (also known as Equitable Threat Score). This is an interesting indicator of performance because it measures the fraction of observed and/or predicted events that were correctly predicted, adjusted for hits (correctly predicted events) associated with random chance. For example, it is easier to correctly predict rain occurrence in a wet climate than in a dry climate. In other words, GSS can answer how well the predicted event occurrences correspond to the observed events (accounting for correct predictions appearing due to chance). The Gilbert Skill Score can be calculated as follows:\n",
    "\n",
    "$GSS = \\frac{ \\text{hits} - \\text{hits}_{random}}{ \\text{hits} + \\text{misses} + \\text{false alarms} - \\text{hits}_{random} } $\n",
    "\n",
    "where the random hits can ba calculated as:\n",
    "\n",
    "$ \\text{hits}_{random} = \\frac{ (\\text{hits}+\\text{misses} )* (\\text{hits}+\\text{false alarms} )}{total} $.\n",
    "\n",
    "_total_ is sum of all the elements in confusion matrix. \n",
    "\n",
    "Hint: Notice the similarity between GSS and threat score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSS Scores will be :  0.25\n"
     ]
    }
   ],
   "source": [
    "def gss(observation, p_scores, threshold):\n",
    "    pass\n",
    "    result_dict=binary_conf_matrix(observation, p_scores, threshold)  ## Getting confusion matrix dictionary\n",
    "    total=result_dict[\"TP\"]+ result_dict[\"FN\"] + result_dict[\"FP\"]+result_dict[\"TN\"]  ## total of confusion matrix\n",
    "    hits_random=(result_dict[\"TP\"]+result_dict[\"FN\"])*(result_dict[\"TP\"]+result_dict[\"FP\"])/total ## Random hits calculation\n",
    "    GSS=(result_dict[\"TP\"] - hits_random)/(result_dict[\"TP\"]+result_dict[\"FN\"]+result_dict[\"FP\"]-hits_random)  ## GSS calculation\n",
    "    return GSS \n",
    "\n",
    "print(\"GSS Scores will be : \", gss(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Bonus Question (10 points)\n",
    "\n",
    "Your last task is to determine an optimal threshold based on prediction scores, observations, and a given performance measure. Create a function called `pick_threshold` which will pick the best prediction score threshold (that will return the highest performance measure based on the given performance metric). Hint: Python allows you to pass a (performance measure) function (such as `tss`, `csi`, or `f1score`) to `pick_threshold`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold value wrt to GSS score is : 0.43\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def pick_threshold(observation, p_scores, mfunc):\n",
    "    '''Finds the prediction score threshold that returns the highest performance based on a given measure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observation: list\n",
    "        list of observations (1 is there is event, 0 is there is no event)\n",
    "    p_scores: list\n",
    "        list of prediction scores (scores vary between 0 and 1)\n",
    "    mfunc: function\n",
    "        one of the performance measurement functions that will take (observation, p_scores, threshold) as\n",
    "        the list of arguments. (Hint: one can call mfunc(observation, p_scores, threshold) within the \n",
    "        function scope.)\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        threshold value that gives the best performance based on mfunc\n",
    "    \n",
    "    '''\n",
    "    pass\n",
    "    score_ls=[]  ## Defining a list which will store respective evaluation score for each threshold\n",
    "    threshold=np.arange(0,1,0.01).tolist() ## Defining thresholds values to check\n",
    "    \n",
    "    for t in threshold:\n",
    "        x=mfunc(observation,p_scores,t) ## Getting evaluation metric value for each threshold\n",
    "        score_ls.append(x)\n",
    "    #print (score_ls)\n",
    "    max_value = max(score_ls)  ## Selecting maximum value of score\n",
    "    i = score_ls.index(max_value)  ## Finding index of maximum score, which will give us the value of respective threshold\n",
    "    return threshold[i]\n",
    "\n",
    "print(\"Best threshold value wrt to GSS score is :\",pick_threshold(Y_test, P_scores, gss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can play around with the step size while creating the threshold values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: IUFS Implementation\n",
    "\n",
    "In this part, you are advised to use the car evaluation dataset. The given dataset contains six descriptive features and a target variable. Each of those are ordinal scale, categorical variables. The name of the target feature is 'evaluation'.\n",
    "\n",
    "Note here that you are expected to write your own code for the feature selection method. Therefore, DO NOT COPY AND PASTE CODE OR USE LIBRARY FUNCTIONS. The goal of the homework is not to see if you can call library functions but to have you practice with the impurity measures and feature selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1728 entries, 0 to 1727\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   buying      1728 non-null   object\n",
      " 1   maint       1728 non-null   object\n",
      " 2   doors       1728 non-null   object\n",
      " 3   persons     1728 non-null   object\n",
      " 4   lug_boot    1728 non-null   object\n",
      " 5   safety      1728 non-null   object\n",
      " 6   evaluation  1728 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 94.6+ KB\n"
     ]
    }
   ],
   "source": [
    "edf = pd.read_csv('careval.csv')\n",
    "# edf.head()\n",
    "edf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a feature selection method called IUFS (impurity-based univariate feature selection), which will select the most informative features with a univariate filter feature selection schema. This feature selection method will take the dataset, name of the target variable, number of features to be selected (k) and the measure of impurity as an input, and will output the names of k best features based on the information gain. You are expected to implement information gain, entropy and Gini index functions. Note here that this will be a univariate selection, which means that you need to test the features individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Question 1 (10 points)\n",
    "Implement a function that calculates entropy for a feature in a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of feature buying: 2.0\n",
      "Entropy of feature evaluation; 1.2057409700121753\n"
     ]
    }
   ],
   "source": [
    "# entropy (H)\n",
    "import math\n",
    "def entropy(feature, dataset):\n",
    "    \"\"\"Calculates the entropy of a feature in a given dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature: str\n",
    "        name of the feature\n",
    "    dataset: pd.DataFrame\n",
    "        dataframe for the dataset\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        entropy for the feature in the dataset\n",
    "    \"\"\"\n",
    "    pass\n",
    "    result_df=pd.DataFrame()\n",
    "    probablity=dataset[feature].value_counts(normalize=True)  ## Probability of each class for feature\n",
    "    result_df=pd.DataFrame({'Class':probablity.index, 'Probability':probablity.values}) ## Converting into dataframe\n",
    "    result_df[\"Log_P\"]= np.log2(result_df['Probability']) ## Finding log of each probability value\n",
    "    result_df[\"Entropy_element\"]=result_df[\"Log_P\"]*result_df[\"Probability\"] ## Multiplying log(p) * P\n",
    "    return -1*result_df[\"Entropy_element\"].sum()  ## Summing all values to get Shannon Entropy\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Entropy of feature buying:\",entropy('buying', edf))\n",
    "print(\"Entropy of feature evaluation;\",entropy('evaluation', edf) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Question 3 (10 points)\n",
    "Implement a function that calculates gini index for a feature in a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gini Index of evaluation feature is : 0.4572837630744171\n"
     ]
    }
   ],
   "source": [
    "# gini index (Gini)\n",
    "\n",
    "def gini(feature, dataset):\n",
    "    \"\"\"Calculates the gini index of a feature in a given dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature: str\n",
    "        name of the feature\n",
    "    dataset: pd.DataFrame\n",
    "        dataframe for the dataset\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        gini index for the feature in the dataset\n",
    "    \"\"\"\n",
    "    pass\n",
    "    result_df=pd.DataFrame()\n",
    "    probablity=dataset[feature].value_counts(normalize=True) ## Probability of each class for feature\n",
    "    result_df=pd.DataFrame({'Class':probablity.index, 'Probability':probablity.values}) ## Converting into dataframe\n",
    "    result_df[\"Squared_Prob\"]=result_df[\"Probability\"]*result_df[\"Probability\"] ## Finding p^2\n",
    "    return 1- (result_df[\"Squared_Prob\"].sum()) ## Return Gini Index\n",
    "    \n",
    "print(\"The Gini Index of evaluation feature is :\",gini('evaluation', edf) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Question 3 (10 points)\n",
    "Implement a function that calculates information gain (IG) for a feature in a given dataset and the target variable. This function is also expected take a measure of impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods={\"gini\":gini,\"entropy\":entropy}## Creating dictionary to call function as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for Buying Feature is: 0.014286077889231918\n"
     ]
    }
   ],
   "source": [
    "# information gain (IG)\n",
    "\n",
    "def IG(feature, target, dataset, measure):\n",
    "    \"\"\"Calculates the information gain of a feature for a given target variable and a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature: str\n",
    "        name of the feature\n",
    "    target: str\n",
    "        name of the target variable\n",
    "    dataset: pd.DataFrame\n",
    "        dataframe for the dataset\n",
    "    measure: str ('entropy' or 'gini')\n",
    "        measure of impurity to be used\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        information gain for the feature in the dataset for a given target variable\n",
    "    \"\"\"\n",
    "    pass\n",
    "    H = methods[measure](target,dataset)## Measure of impurity of target feature\n",
    "    rem=0\n",
    "    len_dataset=dataset.shape[0]\n",
    "    #print(len_dataset)\n",
    "    for value in dataset[feature].unique():  ## For each iteration, split the dataset based on class level\n",
    "        df=pd.DataFrame()\n",
    "        df=dataset.loc[dataset[feature]==value] ## Splitting the dataset based on level of feature\n",
    "        entropy_wrt_target=methods[measure](target,df) ## Getting measure of impurity w.r.t target class ( Entropy/ Gini)\n",
    "        fraction=df.shape[0]/len_dataset   ## Weighted average\n",
    "        weighted_entropy= entropy_wrt_target *fraction  \n",
    "        rem=rem + weighted_entropy  ## Weighted average sum of measure of impurity\n",
    "        \n",
    "    IG=H-rem  ## Information Gain\n",
    "    return IG\n",
    "\n",
    "print(\"Information Gain for Buying Feature is:\",IG('buying','evaluation', edf, 'gini') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Question 4 (20 points)\n",
    "Implement the IUFS feature selection as a function that will select k most informative features using information gain based on a target variable. This function will also take a measure of impurity. It will return a list of k feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Column_Name        IG\n",
      "0      safety  0.262184\n",
      "0     persons  0.219663\n",
      "0      buying  0.096449\n",
      "0       maint  0.073704\n",
      "0    lug_boot  0.030008\n",
      "0       doors  0.004486\n",
      "Two features with highest information gain based on Entropy are: ['safety', 'persons']\n"
     ]
    }
   ],
   "source": [
    "def IUFS(target, dataset, k, measure):\n",
    "    \"\"\"Finds k most informative features in the given dataset based on the target variable\n",
    "        using information gain with the selected measure.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    target: str\n",
    "        name of the target variable\n",
    "    dataset: pd.DataFrame\n",
    "        dataframe for the dataset\n",
    "    k: int\n",
    "        number of features to return, must be less than or equal to number of descriptive features in dataset.\n",
    "        in other words, 0 < k < len(dataset.columns).\n",
    "    measure: str, 'entropy' or 'gini'\n",
    "        measure of impurity\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        returns a list of k feature names, selected based on univariate selection schema\n",
    "    \"\"\"\n",
    "    pass\n",
    "    master_df=pd.DataFrame()\n",
    "    final_list=dataset.columns.tolist()\n",
    "    final_list.remove(target) ## Removing target feature from column list\n",
    "    #print(final_list)\n",
    "    for col in final_list:\n",
    "        temp_df=pd.DataFrame()\n",
    "        temp_df[\"Column_Name\"]=[col]\n",
    "        temp_df[\"IG\"]=[IG(col,target,dataset,measure)]  ## Information Gain for each column\n",
    "        master_df=pd.concat([master_df,temp_df])  ## Combining Columns and Information gain for each feature\n",
    "    master_df_sorted=master_df.sort_values(by='IG', ascending=False) ## Sorting in Descending order based on Information gain\n",
    "    print(master_df_sorted)\n",
    "    column_list=master_df_sorted[\"Column_Name\"].head(k).to_list()  ## Choosing first K columns \n",
    "    return column_list\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "print(\"Two features with highest information gain based on Entropy are:\",IUFS('evaluation', edf, 2, measure='entropy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Bonus Question (10 points)\n",
    "Improve the IUFS by including an option for gain ratio. Gain ratio is an alternative to information gain and can be used with either of the Gini index or entropy measures. First implement gain ratio (GR), then implement the updated version of IUFS function (IUFS2), which will take a selection metric ('IG' for information gain or 'GR' for gain ratio) as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain ratio for buying feature is : 0.019048103852309223\n"
     ]
    }
   ],
   "source": [
    "def GR(feature, target, dataset, measure):\n",
    "    \"\"\"Calculates the gain ratio of a feature for a given target variable and a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature: str\n",
    "        name of the feature\n",
    "    target: str\n",
    "        name of the target variable\n",
    "    dataset: pd.DataFrame\n",
    "        dataframe for the dataset\n",
    "    measure: str ('entropy' or 'gini')\n",
    "        measure of impurity to be used\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        gain ratio for the feature in the dataset for a given target variable\n",
    "    \"\"\"\n",
    "    pass\n",
    "    Information_gain=IG(feature, target, dataset, measure)  ## Getting information gain for the faeture\n",
    "    info_stored=methods[measure](feature, dataset)  ## Information stored by a feature\n",
    "    GR=Information_gain/info_stored  ## GR of the feature\n",
    "    return GR \n",
    "\n",
    "print(\"Gain ratio for buying feature is :\",GR('buying','evaluation', edf, 'gini') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_2={\"IG\":IG,\"GR\":GR} ## Dictionary to call function as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Column_Name      gain\n",
      "0      safety  0.115191\n",
      "0     persons  0.106899\n",
      "0      buying  0.019048\n",
      "0       maint  0.015669\n",
      "0    lug_boot  0.007854\n",
      "0       doors  0.002073\n",
      "Two most informative feature based on Gain Ratio are: ['safety', 'persons']\n"
     ]
    }
   ],
   "source": [
    "def IUFS2(target, dataset, k, measure='entropy', gain='IG'):\n",
    "    \"\"\"Finds k most informative features in the given dataset based on the target variable\n",
    "        using information gain with the selected measure.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    target: str\n",
    "        name of the target variable\n",
    "    dataset: pd.DataFrame\n",
    "        dataframe for the dataset\n",
    "    k: int\n",
    "        number of features to return, must be less than or equal to number of descriptive features in dataset.\n",
    "        in other words, 0 < k < len(dataset.columns).\n",
    "    measure: str, 'entropy' or 'gini'\n",
    "        measure of impurity\n",
    "    gain: str, 'IG' or 'GR'\n",
    "        feature selection metric ('IG' for information gain, 'GR' for gain ratio)\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        returns a list of k feature names, selected based on univariate selection schema\n",
    "    \"\"\"\n",
    "    pass\n",
    "    master_df=pd.DataFrame()\n",
    "    final_list=dataset.columns.tolist()\n",
    "    final_list.remove(target)\n",
    "    #print(final_list)\n",
    "    for col in final_list:\n",
    "        temp_df=pd.DataFrame()\n",
    "        temp_df[\"Column_Name\"]=[col]\n",
    "        temp_df[\"gain\"]=[methods_2[gain](col,target,dataset,measure)] ## To calculate IG/GR\n",
    "        master_df=pd.concat([master_df,temp_df])\n",
    "    master_df_sorted=master_df.sort_values(by='gain', ascending=False)\n",
    "    print(master_df_sorted)\n",
    "    column_list=master_df_sorted[\"Column_Name\"].head(k).to_list()\n",
    "    return column_list\n",
    "\n",
    "print(\"Two most informative feature based on Gain Ratio are:\",IUFS2('evaluation', edf, 2, measure='gini', gain='GR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
